{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Various Parts of HMM\n",
    "References:\n",
    "- <https://www.cs.sjsu.edu/~stamp/RUA/HMM.pdf> - This explains the MM + HMM model with good motivation. It also describes the 3 HMM problems and their solutions but does not explain their derivations.\n",
    "- <https://en.wikipedia.org/wiki/Hidden_Markov_model> - This has further references and links to the solutions used for solving the HMM problems. Some (like the entry on the `forward algotithm`) are well written and can make things clearer.\n",
    "- <http://www.robots.ox.ac.uk:5000/~vgg/rg/papers/hmm.pdf> - The seminal paper on HMMs for speech recognition by Rabiner `89.\n",
    "\n",
    "My aim here is to derive the solutions for each of the HMM problems and apply them to toy examples. I've tried to be as lucid with the Math, code and the derivations as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Toy example 1**\n",
    "\n",
    "- Two hidden states : [1_Hot, 2_Cold]\n",
    "- Three observed states : [0_Small, 1_Medium, 2_Large]\n",
    "- Init Probability : $\\pi = [0.6, 0.4]$\n",
    "- Transition Matrix $A$\n",
    "\n",
    "| t-1 => t | s0 | s_hot | s_cold |\n",
    "|----------|----|-------|--------|\n",
    "| **s0**       | 0  | 0.6   | 0.4    |\n",
    "| **s_hot**    | 0  | 0.7   | 0.3    |\n",
    "| **s_cold**   | 0  | 0.4   | 0.6    |\n",
    "\n",
    "\n",
    "- Emission Matrix $B$\n",
    "\n",
    "| zt => xt | 0_small | 1_medium | 2_large |\n",
    "|----------|---------|----------|---------|\n",
    "| s0       | NA       | NA        | NA       |\n",
    "| s_hot    | 0.1     | 0.4      | 0.5     |\n",
    "| s_cold   | 0.7     | 0.2      | 0.1     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocStates = 2\n",
    "vocObs = 3\n",
    "\n",
    "trA = np.array(\n",
    "    [\n",
    "        [0, 0.6, 0.4],\n",
    "        [0, 0.7, 0.3],\n",
    "        [0, 0.4, 0.6]\n",
    "    ]\n",
    ")\n",
    "\n",
    "emB = np.array(\n",
    "    [\n",
    "        [None, None, None],\n",
    "        [0.1, 0.4, 0.5],\n",
    "        [0.7, 0.2, 0.1]\n",
    "    ]\n",
    ")\n",
    "\n",
    "obX = [0, 1, 0, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "Given $A, B, \\pi$ and a observed sequence $\\vec{x} = (0, 1, 0, 2)$, what is the probability of observing this sequence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As derived, for a sequence $\\vec x$ given $A, B$, we can arrive at the following:\n",
    "$$\n",
    "P(\\vec x; A,B) = \\sum_{\\vec z} P(\\vec x, \\vec z; A,B) \\\\\n",
    "= \\sum_{\\vec z} \\big(\\prod_{t=1}^T B_{z_t, x_t}\\big) \\big(\\prod_{t=1}^T A_{z_{t-1}, z_t}\\big) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution involves considering every possible state assignment combination for the output sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 1, 1) 0.000412 0.0004116\n",
      "(1, 1, 1, 2) 3.5e-05 0.00044688\n",
      "(1, 1, 2, 1) 0.000706 0.00115248\n",
      "(1, 1, 2, 2) 0.000212 0.00136416\n",
      "(1, 2, 1, 1) 5e-05 0.0014145599999999998\n",
      "(1, 2, 1, 2) 4e-06 0.0014188799999999998\n",
      "(1, 2, 2, 1) 0.000302 0.0017212799999999997\n",
      "(1, 2, 2, 2) 9.1e-05 0.0018119999999999998\n",
      "(2, 1, 1, 1) 0.001098 0.0029095999999999996\n",
      "(2, 1, 1, 2) 9.4e-05 0.0030036799999999995\n",
      "(2, 1, 2, 1) 0.001882 0.004885279999999999\n",
      "(2, 1, 2, 2) 0.000564 0.005449759999999999\n",
      "(2, 2, 1, 1) 0.00047 0.005920159999999999\n",
      "(2, 2, 1, 2) 4e-05 0.005960479999999999\n",
      "(2, 2, 2, 1) 0.002822 0.008782879999999998\n",
      "(2, 2, 2, 2) 0.000847 0.009629599999999999\n",
      "0.009629599999999999\n"
     ]
    }
   ],
   "source": [
    "total_prob = 0\n",
    "for i in range(1, 3):\n",
    "    for j in range(1, 3):\n",
    "        for k in range(1, 3):\n",
    "            for l in range(1, 3):\n",
    "                zprob = 1.0\n",
    "                zseq = (i, j, k, l)\n",
    "                for t, (z, x) in enumerate(zip(zseq, obX)):\n",
    "                    _b = emB[z, x]\n",
    "                    \n",
    "                    if t == 0:\n",
    "                        zprev = 0\n",
    "                    else:\n",
    "                        zprev = zseq[t-1]\n",
    "                    \n",
    "                    _a = trA[zprev, z]\n",
    "                    \n",
    "                    zprob = zprob * _a * _b\n",
    "                    \n",
    "                total_prob += zprob\n",
    "                print zseq, round(zprob, 6), total_prob\n",
    "                \n",
    "print total_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution is naive. We are iterating over every combination of $\\vec z$ over time. Let's see if it can be done faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to find $P(\\vec x)$ (matrices $A$, $B$ omitted for brevity). This can be expressed as\n",
    "$$\n",
    "P(\\vec x) = \\sum_{z_t} P(z_t, \\vec x)  \\tag0\n",
    "$$\n",
    "\n",
    "The equation above is true for any expression. Let's now try to expand $P(z_t, \\vec x)$ in terms of our emission and transition terms ...\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(z_t, x_{1:t}) &= \\sum_{z_{t-1}} P(z_t, z_{t-1}, x_{1:t}) \\tag1 \\\\\n",
    "&= \\sum_{z_{t-1}} P(x_t, z_t, z_{t-1}, x_{t-1}, x_{t-2}, ... , x_1) \\tag2 \\\\\n",
    "&= \\sum_{z_{t-1}} P(x_t | z_t) P(z_t, z_{t-1}, x_{1:t-1}) \\tag3 \\\\\n",
    "&= \\sum_{z_{t-1}} P(x_t | z_t) P(z_t | z_{t-1}) P(z_{t-1}, x_{1:t-1}) \\tag4 \\\\\n",
    "&= P(x_t | z_t) \\sum_{z_{t-1}} P(z_t | z_{t-1}) P(z_{t-1}, x_{1:t-1}) \\tag5\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In eq 1, we introduce a new hidden state var from previous timestep. Eq 2 simple expands $x_{1:t}$ before applying the Markov assumptions. In eq 3 we apply the emission prob. assumption and in eq 4 we apply the transition prob. assumption. Finally, in eq 5, we can take $P(x_t, z_t)$ out of the summand since we are iterating over values of $z_{t-1}$ only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that there is a recurrence b/w EQ1 and EQ5. If we define $\\alpha_z(t)$ to be $P(z_t, x_{1:t})$, then we can rewrite EQ 5 as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(z_t, x_{1:t}) &= P(x_t | z_t) \\sum_{z_{t-1}} P(z_t | z_{t-1}) P(z_{t-1}, x_{1:t-1}) \\\\\n",
    "\\rightarrow \\alpha_z(t) &= P(x_t | z_t) \\sum_{z_{t-1}} P(z_t | z_{t-1}) \\alpha_z(t-1) \\\\\n",
    "&= B_{z_t, x_t} \\sum_{z_{t-1}} A_{z_t, z_{t-1}} \\alpha_z(t-1)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This expression only involves summing over all possible states of $z_i$, which in our example is 2. Our original expression EQ 0, thus becomes\n",
    "\n",
    "$$\n",
    "P(\\vec x) = \\sum_{z_t} P(z_t, \\vec x) = \\sum_z \\alpha_z(t = T)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are no longer considering all possible combinations of the entire sequence of hidden states, but rather, the possible values of $z$, which in our example is 2. And to calculate $\\alpha(T)$, we will have to iterate over all timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_toy = \"\"\"\n",
    "====\n",
    "Init\n",
    "====\n",
    "_|_H_|_L_\n",
    "*|0.5|0.5\n",
    "\n",
    "============\n",
    "Transmission\n",
    "============\n",
    "_|_H_|_L_\n",
    "H|0.5|0.5\n",
    "L|0.4|0.6\n",
    "\n",
    "========\n",
    "Emission\n",
    "========\n",
    "_|_A_|_C_|_G_|_T_\n",
    "H|0.2|0.3|0.3|0.2\n",
    "L|0.3|0.2|0.2|0.3\n",
    "\n",
    "========\n",
    "Observed\n",
    "========\n",
    "[G G C A]\n",
    "\"\"\"\n",
    "states = ['H', 'L']\n",
    "genes = ['A', 'C', 'G', 'T']\n",
    "\n",
    "inits = [0.5, 0.5]\n",
    "\n",
    "trans = np.array(\n",
    "    [\n",
    "        [0.5, 0.5],\n",
    "        [0.4, 0.6]\n",
    "    ]\n",
    ")\n",
    "emmit = np.array(\n",
    "    [\n",
    "        [0.2, 0.3, 0.3, 0.2],\n",
    "        [0.3, 0.2, 0.2, 0.3]\n",
    "    ]\n",
    ")\n",
    "\n",
    "obsX = [2, 2, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0, 0) 0.0003375 0.0003375\n",
      "(0, 0, 0, 1) 0.00050625 0.00084375\n",
      "(0, 0, 1, 0) 0.00018 0.00102375\n",
      "(0, 0, 1, 1) 0.0004049999999999999 0.00142875\n",
      "(0, 1, 0, 0) 0.00018 0.00160875\n",
      "(0, 1, 0, 1) 0.00027 0.00187875\n",
      "(0, 1, 1, 0) 0.000144 0.00202275\n",
      "(0, 1, 1, 1) 0.000324 0.00234675\n",
      "(1, 0, 0, 0) 0.00018000000000000004 0.00252675\n",
      "(1, 0, 0, 1) 0.00027 0.0027967499999999998\n",
      "(1, 0, 1, 0) 9.600000000000004e-05 0.00289275\n",
      "(1, 0, 1, 1) 0.00021600000000000005 0.00310875\n",
      "(1, 1, 0, 0) 0.000144 0.00325275\n",
      "(1, 1, 0, 1) 0.00021600000000000002 0.00346875\n",
      "(1, 1, 1, 0) 0.00011520000000000001 0.0035839500000000002\n",
      "(1, 1, 1, 1) 0.0002592 0.00384315\n"
     ]
    }
   ],
   "source": [
    "## Priyam, forget whatever shit you wrote on top.\n",
    "## We define `\\alpha` for every timestep AND every hidden state ==> `\\alpha_i(t)`.\n",
    "total_prob = 0\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        for k in range(2):\n",
    "            for l in range(2):\n",
    "                prob = 1.0\n",
    "                zstates = (i, j, k, l)\n",
    "                \n",
    "                for t, (z, x) in enumerate(zip(zstates, obsX)):\n",
    "                    if t == 0:\n",
    "                        prob = prob * inits[z] * emmit[z, x]\n",
    "                    else:\n",
    "                        prob = prob * trans[zstates[t-1], z] * emmit[z, x]\n",
    "                \n",
    "                total_prob += prob\n",
    "                print zstates, prob, total_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====\n",
      "Init\n",
      "====\n",
      "_|_H_|_L_\n",
      "*|0.5|0.5\n",
      "\n",
      "============\n",
      "Transmission\n",
      "============\n",
      "_|_H_|_L_\n",
      "H|0.5|0.5\n",
      "L|0.4|0.6\n",
      "\n",
      "========\n",
      "Emission\n",
      "========\n",
      "_|_A_|_C_|_G_|_T_\n",
      "H|0.2|0.3|0.3|0.2\n",
      "L|0.3|0.2|0.2|0.3\n",
      "\n",
      "========\n",
      "Observed\n",
      "========\n",
      "[G G C A]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print _toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0038431500000000005\n",
      "Matches brute force!\n"
     ]
    }
   ],
   "source": [
    "for t,x in enumerate(obsX):\n",
    "    if t == 0:\n",
    "        alpha_prev = []\n",
    "        for i in range(2):\n",
    "            alpha_prev.append(inits[i] * emmit[i, x])\n",
    "            \n",
    "    else:\n",
    "        alpha_new = []\n",
    "        for i in range(2):\n",
    "            _jsum = 0\n",
    "            for j in range(2):\n",
    "                _jsum += alpha_prev[j] * trans[j, i]\n",
    "            \n",
    "            alpha_new.append(_jsum * emmit[i, x])\n",
    "        \n",
    "        alpha_prev = deepcopy(alpha_new)\n",
    "        \n",
    "print sum(alpha_new)\n",
    "assert round(total_prob, 6) == round(sum(alpha_new), 6)\n",
    "print \"Matches brute force!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have it working, let's try deriving the *alpha-pass* algorithm again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following EQ 5, we can express the joint probability $P(z_t, x_{1:t})$ as \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(z_t, x_{1:t}) &= P(x_t | z_t) \\sum_{z_{t-1}} P(z_t | z_{t-1}) P(z_{t-1}, x_{1:t-1}) \\tag6\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, $z_t$ is simply a random variable whose value exists in the set of state values $S$; the joint distribution \"table\" is defined for all possible values that $z_t$ can have. Concretely, we can define the joint probability for a particular state value $s_i$ as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(z_t=s_i, x_{1:t}) &= P(x_t | z_t=s_i) \\sum_{s_j \\in S} P(z_t = s_i | z_{t-1} = s_j) \\times P(z_{t-1} = s_j, x_{1:t-1}) \\tag7\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this view, we can define $\\alpha_t(s_i)$, as the probability of the observed partial sequnce upto time $t$ where $z_t = s_i$ :\n",
    "\n",
    "$$\n",
    "\\alpha_t(s_i) = P(z_t = s_i, x_{1:t}) \\tag8\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can re-write EQ7 in terms of $\\alpha, A, B$:\n",
    "\n",
    "$$\n",
    "\\alpha_t(s_i) = B_{s_i, x_t} \\sum_{s_j \\in S} A_{s_j, s_i} \\times \\alpha_{t-1}(s_j) \\tag9\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base-case, for this recursive definition is for the first observation $x_1$. This is simply the probability of observing $x_1$ for all possible state assignment to $z_1$.\n",
    "\n",
    "$$\n",
    "\\alpha_1(s_i) = B_{s_i, x_1} \\pi_{s_i} \\tag{10}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using EQs 8, 9 and 10, we can express the probability of observing any sequnce $x_{1:T}$ as \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(x_{1:T}) &= \\sum_{s_i \\in S} P(x_{1:T}, z_T=s_i) \\\\\n",
    "&= \\sum_{s_i \\in S} \\alpha_T(s_i) \\tag{11}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally! We have now reduced the time-complexity from $O(|S|^T)$ to $O(|S|^2T)$. How?\n",
    "- To arrive at $\\alpha_T$, we will have to loop over all possible values of $T$.\n",
    "```python\n",
    "for t,x in enumerate(obsX):\n",
    "```\n",
    "\n",
    "- At every iteration, we will compute the intermediate value $\\alpha_t(s_i)$ for all $i$, which will require a sum over every state value $j$ from the previously computer $\\alpha_{t-1}$ (from EQ 9) -- hence the $|S|^2$.\n",
    "```python\n",
    "        alpha_new = []\n",
    "        for i in range(2): ## For current $\\alpha_t$\n",
    "            _jsum = 0\n",
    "            for j in range(2): ## From previous $\\alpha_{t-1}$\n",
    "                _jsum += alpha_prev[j] * trans[j, i]\n",
    "            \n",
    "            alpha_new.append(_jsum * emmit[i, x])\n",
    "        \n",
    "        alpha_prev = deepcopy(alpha_new)\n",
    "```\n",
    "\n",
    "Once the last `alpha_new` has been computed, the joint is simply the sum of probabilities over the states : `return sum(alpha_new)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (a)\n",
    "Given a sequence of time $t$, what is the distribution over hidden states $P(z_t)$ at time $t$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find $P(z_t | x_{1:t})$. This can be done easily using the result from the `forward algorithm`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "P(z_t = s_i | x_{1:t}) &= \\frac{P(z_t = s_i, x_{1:t})}{P(x_{1:t})} \\tag{12} \\\\\n",
    "&= \\frac{\\alpha_t(s_i)}{\\sum_{s_j}\\alpha_t(s_j)} \\tag{13}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (b)\n",
    "Finding $P(z_t = s_i | x_{1:T})$ for any arbitrary $t \\in [1, T]$ is not trivial. Deriving this to the final \"optimized form\" took a while because it was not apparent what terms to factorize in the joint expression.\n",
    "\n",
    "We begin by expressing $P(z_t = s_i | x_{1:T}) \\propto P(z_t = s_i, x_{1:T})$. The equation is complete with $P(x_{1:T})$ in the denominator as in EQ.12, 13. We ignore that term for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "P(z_t = s_i | x_{1:T}) &\\propto P(z_t = s_i, x_{1:T}) \\\\\n",
    "&= P(z_t = s_i, x_{1:t}, x_{t+1:T}) \\tag{14} \\\\\n",
    "&= P(x_{t+1:T} | z_t=s_i, x_{1:t}) \\ P(z_t = s_i, x_{1:t}) \\tag{15}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In EQ. 14 15, we are trying to factorize the joint into two terms: the second term we have solved already in EQ.12. We can simplify the first term by exploiting the fact that the probability of the future observations $x_{t+1:T}$ is independent of the past observations $x_{1:t}$ given the current hidden state $z_t = s_i$. The following diagram, will make it clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](hmm-independence.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can rewrite it to $P(x_{t+1 : T}|z_t = s_i) \\times \\alpha_t(s_i)$. Now let's estimate the first term.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(x_{t+1 : T}|z_t = s_i) &= \\frac{P(x_{t+1 : T}, z_t = s_i)}{P(z_t = s_i)} \\\\ \\\\\n",
    "&= \\sum_{s_j \\in S} P(x_{t+1:T}, z_t = s_i, z_{t+1}=s_j) / P(z_t = s_i) \\tag{16} \\\\ \\\\\n",
    "&= \\sum_{s_j \\in S} P(x_{t+1}, x_{t+2:T}, z_t = s_i, z_{t+1}=s_j) / P(z_t = s_i) \\tag{17} \\\\ \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EQ.16 is trivial - we introduce a new state variable $z_{t+1}$. In EQ. 17, we breakdown the observed states into $x_{t+1}, x_{t+2:T}$. This will allow us to factorize the expression as a recursion of FUTURE time-steps. Continuing..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "&= \\sum_{s_j \\in S} P(x_{t+2:T} | x_{t+1}, z_t = s_i, z_{t+1}=s_j) \\times P(x_{t+1}, z_t = s_i, z_{t+1}=s_j) / P(z_t = s_i) \\tag{18} \\\\ \\\\\n",
    "&= \\sum_{s_j \\in S} P(x_{t+2:T} | z_{t+1}=s_j) \\times P(x_{t+1} | z_t = s_i, z_{t+1}=s_j) \\times P(z_t = s_i, z_{t+1}=s_j) / P(z_t = s_i) \\tag{19} \\\\ \\\\\n",
    "&= \\sum_{s_j \\in S} P(x_{t+2:T} | z_{t+1}=s_j) \\times P(x_{t+1} | z_{t+1}=s_j) \\times P(z_{t+1} = s_j | z_t = s_i) \\times P(z_t = s_i) / P(z_t = s_i) \\tag{20} \\\\ \\\\\n",
    "&= \\sum_{s_j \\in S} P(x_{t+2:T} | z_{t+1}=s_j) \\times B_{x_{t+1}, s_j} \\times A_{s_i, s_j} \\tag{21}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots going here. \n",
    "- In EQ 19, the first term is a result of applying the concept from our last diagram -- the future observations $x_{t+2:T}$ only depend on the current hidden state $z_{t+1} = s_j$. The simplied form is the first term in EQ 20.\n",
    "- The remaining terms in EQ 19 are factors of the second joint prob term in EQ 18.\n",
    "- In EQ 20, for the second term, we apply the emission independence assumption. The next two terms in the equation are factors of the joint $P(z_t = s_i, z_{t+1} = s_j)$.\n",
    "- The last two terms in EQ 20 cancel each other and the final form in terms of our emission and transition matrices is in EQ 21."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And *now*, we finally have our recursive relation. At any time $t$ and a particular state assignment $s_i$ we define $\\beta_t(s_i) = P(x_{t+1 : T} | z_t = s_i)$. This is the probability of all future observations, given the current hidden state. Appropriately,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(x_{t+1 : T}|z_t = s_i) &= \\sum_{s_j \\in S} P(x_{t+2:T} | z_{t+1}=s_j) \\times B_{x_{t+1}, s_j} \\times A_{s_i, s_j} \\\\ \\\\\n",
    "\\implies \\beta_t(s_i) &= \\sum_{s_j \\in S} \\beta_{t+1}(s_j) \\times B_{x_{t+1}, s_j} \\times A_{s_i, s_j} \\tag{22}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this in place, we can define the base-case at the last timestep $T$ as $\\beta_T(s_i \\in S) = 1$, since there are no observations after time $T$. We then work back starting from $T$ and use the future $\\beta$ values in each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using EQs 12 and 14, we can now complete the expression $P(z_t = s_i | x_{1:T})$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(z_t = s_i | x_{1:T}) &= \\frac{P(z_t = s_i, x_{1:T})}{P(x_{1:T})} \\tag{joint} \\\\ \\\\\n",
    "&= \\frac{P(z_t = s_i, x_{1:t}, x_{t+1:T})}{P(x_{1:T})} \\tag{observed seq time split} \\\\ \\\\\n",
    "&= \\frac{P(x_{t+1:T} | z_t=s_i, x_{1:t}) \\ P(z_t = s_i, x_{1:t})}{P(x_{1:T})} \\tag{factoring for $\\alpha, \\beta$} \\\\ \\\\\n",
    "&= \\frac{P(x_{t+1:T} | z_t=s_i) \\ P(z_t = s_i, x_{1:t})}{P(x_{1:T})} \\tag{\"independence\" from the diagram} \\\\ \\\\\n",
    "&= \\frac{\\beta_t(s_i) \\ \\alpha_t(s_i)}{P(x_{1:T})} \\tag{23}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, I have introduce yet another toy example before we begin. This ones from the [Forward-Backward Algorithm's Wiki page](https://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ('Healthy', 'Fever')\n",
    "inits = [0.6, 0.4]\n",
    "\n",
    "trans = np.array(\n",
    "    [\n",
    "        [0.7, 0.3],\n",
    "        [0.4, 0.6]\n",
    "    ]\n",
    ")\n",
    "\n",
    "outputs = ['normal', 'cold', 'dizzy']\n",
    "emmit = np.array(\n",
    "    [\n",
    "        [0.5, 0.4, 0.1],\n",
    "        [0.1, 0.3, 0.6]\n",
    "    ]\n",
    ")\n",
    "\n",
    "obsX = [0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of observing sequence ['normal', 'cold', 'dizzy'] 0.03628\n",
      "Max prob 0.01512\n",
      "2 ahead_beta [1.0, 1.0]\n",
      "1 ahead_beta [0.25, 0.4]\n",
      "0 ahead_beta [0.10599999999999998, 0.112]\n",
      "[0.8765159867695699, 0.12348401323043003] 0 0.8765159867695699\n",
      "[0.6229327453142228, 0.37706725468577734] 0 0.6229327453142228\n",
      "[0.2121278941565601, 0.7878721058434399] 1 0.7878721058434399\n"
     ]
    }
   ],
   "source": [
    "## forward pass\n",
    "alphas = []\n",
    "for t, x in enumerate(obsX):\n",
    "    if t == 0: ## init prev_alpha\n",
    "        prev_alpha = []\n",
    "        for s in range(len(states)):\n",
    "            prev_alpha.append(emmit[s, x] * inits[s])\n",
    "        prev_prob = deepcopy(prev_alpha)\n",
    "            \n",
    "    else:\n",
    "        new_alpha = []\n",
    "        max_prob = []\n",
    "        \n",
    "        for i in range(len(states)):\n",
    "            accum = 0.0\n",
    "            for j in range(len(states)):\n",
    "                accum += prev_alpha[j] * trans[j, i]\n",
    "                \n",
    "            new_alpha.append(accum * emmit[i, x])\n",
    "            \n",
    "            max_prob.append(np.max([prev_prob[j] * trans[j, i] * emmit[i, x] for j in range(len(states))]))\n",
    "        \n",
    "        prev_alpha = deepcopy(new_alpha)\n",
    "        prev_prob = deepcopy(max_prob)\n",
    "    \n",
    "    alphas.append(prev_alpha)\n",
    "            \n",
    "p_obs = sum(new_alpha)\n",
    "print \"Probability of observing sequence\", [outputs[x] for x in obsX], p_obs\n",
    "print \"Max prob\", np.max(max_prob)\n",
    "\n",
    "## backward pass\n",
    "betas = []\n",
    "for t in range(len(obsX))[::-1]:\n",
    "    if t == (len(obsX)-1):\n",
    "        ahead_beta = []\n",
    "        for s in range(len(states)):\n",
    "            ahead_beta.append(1.0)\n",
    "            \n",
    "    else:\n",
    "        x = obsX[t+1]\n",
    "        current_beta = []\n",
    "        for i in range(len(states)):\n",
    "            accum = 0.0\n",
    "            for j in range(len(states)):\n",
    "                accum += trans[i, j] * emmit[j, x] * ahead_beta[j]\n",
    "            \n",
    "            current_beta.append(accum)\n",
    "            \n",
    "        ahead_beta = deepcopy(current_beta)\n",
    "    \n",
    "    print t, \"ahead_beta\", ahead_beta    \n",
    "    betas.insert(0, ahead_beta)\n",
    "\n",
    "## \n",
    "for t in range(len(obsX)):\n",
    "    p_states = [alphas[t][s] * betas[t][s] / p_obs for s in range(len(states))]\n",
    "    print p_states, np.argmax(p_states), np.max(p_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2(c) Viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to find the most likely state sequence, given the observed sequence.\n",
    "\n",
    "$$\n",
    "\\underset{z_{1:T}}{\\text{argmax }} P(z_{1:T} | x_{1:T}) = \\underset{z_{1:T}}{\\text{argmax }} P(z_{1:T}, x_{1:T}) \\tag{joint; denominator is $P(x_{1:T})$}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability expression (*not* the `argmax`) can be factorized very elegantly as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(z_{1:T}, x_{1:T}) &= P(x_1 | z_1) P(z_1) \\prod_{t=2}^T P(x_t | z_t) P(z_t | z_{t-1}) \\tag{24}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the most likely state assignment, we could try all possible combinations of state assignments for the entire sequence and see which combination maximizes EQ.24. This will be in the order of $|S|^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To motivate why Viterbi works, and it's link to Dynamic Programming, consider a simpler problem where you know the best state assignment upto timestep $T-1$. Now, you simply have to pick the link that maximizes the final state assignment. We can extend this logic back to the first state : Viterbi computes the state asignment probabilities at time $t$, which maximize the probability of observing all the data upto time $t$.\n",
    "\n",
    "For the first observation $x_1$, there is only 1 random variable $z_1$, for which we can easily calculate the assignment probabilities along the lines of EQ 24. For the next observation $x_2$, we consider all possible transitions from the previous state variable $z_1$ to the current variable $z_2$; if every state can have $S$ possible assignments, then we consider $S^2$ links. We then pick *that* link to every state assignment $z_2$ which has the highest probability.\n",
    "\n",
    "![](hmm-viterbi.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use EQ.24, to compute the joint probability of the states, upto any time $t$. For $t=1$, this is simply the initialization probability and the observation probability : \n",
    "$$\n",
    "P(z_1 = s_i, x_1) = B_{x_1, s_i}\\times \\pi_{s_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $t = 2$, we can write the joint as follows\n",
    "$$\n",
    "P(z_1, z_2, x_1, x_2) = B_{x_1, z_1}\\pi_{z_1} \\times B_{x_2, z_2} A_{z_2, z_1}\n",
    "$$\n",
    "\n",
    "The first term, can come from the previous step, saving the need to needlessly compute the entire probability sequence every time.\n",
    "\n",
    "The complete algorithm inferes the hidden state sequence in two stages:\n",
    "1. In the first stage, you iterate over all timesteps starting from 1 and keep a track of\n",
    "    - The most probable current state with the probability of \"reaching here and observing the data till this point\".\n",
    "    - The most probable connection to current state from *every* possible previous state.\n",
    "2. In the second stage, we backtrack from the last timestep, selecting the most probable state assignments, through the most likely paths.\n",
    "\n",
    "I follow the notation used by Rabiner for the variables used in Viterbi (`delta` for the state probabilities, and `psi` for the most probable path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden State Sequence: ['Healthy', 'Healthy', 'Fever']\n",
      "State Probabilities ['Healthy', 'Fever']: [[0.3, 0.04000000000000001], [0.084, 0.027], [0.00588, 0.01512]]\n"
     ]
    }
   ],
   "source": [
    "## Viterbi.\n",
    "delta = []\n",
    "psi = []\n",
    "\n",
    "## stage1\n",
    "for t, x in enumerate(obsX):\n",
    "    if t == 0:\n",
    "        _d = []\n",
    "        _p = []\n",
    "        for s in range(len(states)):\n",
    "            _d.append(emmit[s, x] * inits[s])\n",
    "            _p.append(0)\n",
    "            \n",
    "        delta.append(_d)\n",
    "        psi.append(_p)\n",
    "        \n",
    "    else:\n",
    "        _d = []\n",
    "        _p = []\n",
    "        for curr in range(len(states)):\n",
    "            temp = []\n",
    "            for prev in range(len(states)):\n",
    "                temp.append(delta[t-1][prev] * trans[prev, curr] * emmit[curr, x]) ## delta\n",
    "                \n",
    "            _d.append(max(temp))\n",
    "            \n",
    "            temp = []\n",
    "            for prev in range(len(states)):\n",
    "                temp.append(delta[t-1][prev] * trans[prev, curr]) ## psi\n",
    "                \n",
    "            _p.append(np.argmax(temp))\n",
    "            \n",
    "        delta.append(_d)\n",
    "        psi.append(_p)\n",
    "        \n",
    "## stage2\n",
    "decoding = []\n",
    "qt = np.argmax(delta[-1])\n",
    "decoding.append(qt)\n",
    "\n",
    "for t in range(len(obsX))[::-1][:-1]:\n",
    "    qt = psi[t][qt]\n",
    "    decoding.append(qt)\n",
    "    \n",
    "## answer\n",
    "print \"Hidden State Sequence:\", [states[s] for s in reversed(decoding)]\n",
    "print \"State Probabilities ['Healthy', 'Fever']:\", delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results match with the Wiki example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
